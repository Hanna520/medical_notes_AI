{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f02a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT imports\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d14b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Natural Language Toolkit\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd3d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c336f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6e67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054cad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = pd.read_csv(\"data/labels_all.csv\")\n",
    "labels_all.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "text_all = pd.read_csv('data/text_all.csv')\n",
    "text_all.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "#  Create binary labels for id\n",
    "disease_types = ['Hypertriglyceridemia','Venous Insufficiency','Asthma','Gout','OSA','PVD','Gallstones','OA','GERD',\n",
    "                'Depression','Obesity','CHF','Hypercholesterolemia','CAD','Diabetes','Hypertension']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367ddb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the outcome variables (disease types) binary\n",
    "def make_binary(labels_df):\n",
    "    for i in range(16):\n",
    "        # In the disease column, code the disease of interest as 1 and other disease types as 0 to indicate \n",
    "        # whether this row is about this particular disease\n",
    "        labels_df[\"disease_\"+str(i)] = [1 if x==disease_types[i] else 0 for x in labels_df[\"disease\"]]\n",
    "        # In the label column, code Y as 1 and everything else as 0 to indicate whether a disease is present\n",
    "        # (it may or may not be about this particular disease)\n",
    "        labels_df[\"label_\"+str(i)] = [1 if x==\"Y\" else 0 for x in labels_df[\"label\"]] \n",
    "        # Create a \"This_Disease\" column that indicates whether a patient has this particular disease or not\n",
    "        # A patient is coded as having this disease only when (disease is 1) AND (label is 1)\n",
    "        labels_df[\"This_Disease_\"+str(i)] = labels_df[\"disease_\"+str(i)] * labels_df[\"label_\"+str(i)] \n",
    "        labels_df.drop([\"disease_\"+str(i), \"label_\"+str(i)], axis=1, inplace=True)\n",
    "\n",
    "    # Now that all useful info in \"disease\" and \"Label\" are combined in column \"This_Disease\", no longer need disease and label\n",
    "    labels_df.drop(['label', 'disease'], axis=1, inplace=True)\n",
    "    # Duplicates after removing disease and label are due to 0's in the \"disease\" column if the patient had other disease info\n",
    "    labels_df.drop_duplicates(inplace = True)\n",
    "    # Aggregate the binary values into one row for each id\n",
    "    labels_df = labels_df.groupby('id').aggregate('sum').reset_index()\n",
    "    \n",
    "    return(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10a2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = make_binary(labels_all)\n",
    "\n",
    "# Calculate the disease prevalence for each disease type\n",
    "prevalences = (binary_labels.aggregate('sum')/1237).iloc[1:,]\n",
    "\n",
    "# Combine the text data and the labels\n",
    "df = text_all.merge(binary_labels, on=\"id\", how=\"left\")\n",
    "\n",
    "del labels_all, text_all, binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba425b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.iloc[:50,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c6f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rakovski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import the standard English stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97266c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = \"\".join([x for x in text if x not in string.punctuation]) # Remmove punctuations \n",
    "    text = \"\".join([x.lower() for x in text]) # Convert to lower case\n",
    "    text = ' '.join(['' if (x in stopwords_english) else x for x in text.split()]) # Remove stopwords\n",
    "    text = re.sub(\"(\\W|\\d+|\\n)\", \" \", text).strip() # remove spaces, digits and line breaks\n",
    "    text = ' '.join(['' if (len(x) <= 2) else x for x in text.split()]) # Remove short words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb577de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [preprocessing(x) for x in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1d0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change it to stratified split later\n",
    "# x_train,x_test ,y_train,y_test = train_test_split(df['text'], df['This_Disease_14'], random_state = 1, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442178e",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d534f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45726a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\rakovski\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "bert_encoder = hub.KerasLayer(encoder_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6715b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_preprocess([\"Machine Learning is cool!\", \"This model is amazing.\"])\n",
    "# text_inputs = [\"Machine Learning is cool!\", \"This model is amazing.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb86179",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.load(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85bc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: tokenize batches of text inputs.\n",
    "# text_inputs = [tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')]\n",
    "# tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "# tokenized_inputs = [tokenize(segment) for segment in text_inputs]\n",
    "\n",
    "# # Step 3: pack input sequences for the Transformer encoder.\n",
    "# seq_length = 512  \n",
    "# bert_pack_inputs = hub.KerasLayer(\n",
    "#     preprocessor.bert_pack_inputs,\n",
    "#     arguments=dict(seq_length=max_length))  # Optional argument.\n",
    "# encoder_inputs = bert_pack_inputs(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4772c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = bert_encoder(encoder_inputs)\n",
    "# # Neural network layers\n",
    "# l = tf.keras.layers.Dropout(0.3, name=\"dropout\")(outputs['pooled_output'])\n",
    "# l = tf.keras.layers.Dense(128, activation='relu', name=\"dense\")(l)\n",
    "# l = tf.keras.layers.Dropout(0.3, name=\"dropout2\")(l)\n",
    "# y = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# # Use inputs and outputs to construct a final model\n",
    "# model = tf.keras.Model(inputs=[text_inputs], outputs = [y])\n",
    "# model.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c35759c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b36c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5d99532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the model performance in terms of F1 score\n",
    "def evaluate(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    pred = [1 if p>=0.5 else 0 for p in pred]\n",
    "    acc = np.sum(y == pred)/len(y)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = (2*precision*recall)/(precision + recall)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    auc_roc = round(roc_auc_score(y, y_pred),4)\n",
    "    pre, rec, thresholds = precision_recall_curve(y, y_pred)\n",
    "    auc_pr = round(auc(rec, pre),4)\n",
    "    return(auc_roc, auc_pr, acc, precision, recall, specificity, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 481s 16s/step - loss: 0.2673 - auc: 0.4641\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 467s 16s/step - loss: 0.2406 - auc: 0.4310\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 467s 16s/step - loss: 0.2160 - auc: 0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-03a631c9fcc7>:9: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = tp/(tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 500s 17s/step - loss: 0.2828 - auc: 0.5022\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 506s 17s/step - loss: 0.2235 - auc: 0.4595\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 516s 18s/step - loss: 0.2299 - auc: 0.4764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-03a631c9fcc7>:9: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = tp/(tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 478s 16s/step - loss: 0.2606 - auc: 0.4654\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 492s 17s/step - loss: 0.2297 - auc: 0.4572\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 494s 17s/step - loss: 0.2265 - auc: 0.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-03a631c9fcc7>:9: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = tp/(tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 511s 18s/step - loss: 0.2860 - auc: 0.5507\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 570s 20s/step - loss: 0.2269 - auc: 0.4738\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 640s 22s/step - loss: 0.2238 - auc: 0.4829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-03a631c9fcc7>:9: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = tp/(tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 573s 20s/step - loss: 0.3172 - auc: 0.4953\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 605s 21s/step - loss: 0.2170 - auc: 0.5640\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 655s 23s/step - loss: 0.2137 - auc: 0.5444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-03a631c9fcc7>:9: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = tp/(tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "28/29 [===========================>..] - ETA: 20s - loss: 0.2567 - auc: 0.4992"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "\n",
    "epochs = 3\n",
    "max_length = 512\n",
    "batch_size = 32\n",
    "\n",
    "time_1 = time.time()\n",
    "\n",
    "for i in range(16): # Go through each disease type #####################################\n",
    "\n",
    "    event_categorical = df['This_Disease_'+str(i)]\n",
    "    dis_type = disease_types[i]\n",
    "    \n",
    "    time_start = time.time()   \n",
    "\n",
    "    # Split the data into train and test sets with stratification\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.25,random_state=0)\n",
    "    \n",
    "    j = 0\n",
    "    for train_index, test_index in sss.split(X, event_categorical):   \n",
    "              \n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = event_categorical[train_index], event_categorical[test_index]\n",
    "        \n",
    "        j += 1\n",
    "        iteration = \"iter\" + str(j)\n",
    "\n",
    "        # tokenize batches of text inputs.\n",
    "        text_inputs = [tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')]\n",
    "        tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "        tokenized_inputs = [tokenize(segment) for segment in text_inputs]\n",
    "\n",
    "        # pack input sequences for the Transformer encoder.       \n",
    "        bert_pack_inputs = hub.KerasLayer(\n",
    "            preprocessor.bert_pack_inputs,\n",
    "            arguments=dict(seq_length=max_length))  # Optional argument.\n",
    "        encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "        \n",
    "        # BERT model:\n",
    "        outputs = bert_encoder(encoder_inputs)\n",
    "\n",
    "        x = tf.keras.layers.Dropout(0.3, name=\"dropout\")(outputs['pooled_output'])\n",
    "        x = tf.keras.layers.Dense(128, activation='relu', name=\"dense\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.3, name=\"dropout2\")(x)\n",
    "        y = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
    "\n",
    "        # Use inputs and outputs to construct a final model\n",
    "        mymodel = tf.keras.Model(inputs=[text_inputs], outputs = [y])\n",
    "        mymodel.layers[3].trainable = False\n",
    "        mymodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "        time_s = time.time() \n",
    "        mymodel.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "        time_e = time.time() - time_s\n",
    "\n",
    "        # Collect and log evaluation metrics\n",
    "#             auc_roc, auc_pr, acc, precision, recall, specificity, f1 = evaluate(model, x_test, y_test[:,1])\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1 = evaluate(mymodel, x_test, y_test)\n",
    "\n",
    "        with open('other/CNN_Paper_no_embedding_BERT2.csv','a') as fd:\n",
    "            fd.write(f'{dis_type},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{time_e}\\n')\n",
    "        \n",
    "        del x_train, x_test,text_inputs,tokenize,tokenized_inputs,bert_pack_inputs,encoder_inputs,outputs,x,y,mymodel\n",
    "    \n",
    "    # Average metrics\n",
    "    running_time = time.time() - time_start\n",
    "#     logging.info(f',{disease_types[i]}, running time, {running_time}')\n",
    "    with open('other/CNN_Paper_no_embedding_BERT2.csv','a') as fd:\n",
    "        fd.write(f'{dis_type}, 10 iteration running time, {running_time}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58621c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_2 = time.time() - time_1  \n",
    "with open('other/CNN_Paper_no_embedding_BERT2.csv','a') as fd:\n",
    "    fd.write(f'{dis_type}, Total running time, {time_2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11909f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "# from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# evaluate(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes: acc=0.6923076923076923, bert tokenizatin time: , training time: 2177.546822309494 with 0.5 dropout rate\n",
    "# Diabetes: acc=, bert tokenizatin time: , training time: 1586.164499759674  with 0.3 dropout rate, untrainable\n",
    "# Diabetes: acc=0.5384615384615384, training time: 1255, dense layer (128), trainable, lr=0.00001.#error\n",
    "# Diabetes: acc=0.46153846153846156, training time: 1447, dense layer (128),  trainable, lr=default\n",
    "# Diabetes: acc=0.46153846153846156, training time:420, no dense layer,  untrainable, lr=default.\n",
    "# Diabetes: CNN: acc=0.8258064516129032\n",
    "# Diabetes: untrainable, dense 128.\n",
    "# (0.5532,\n",
    "#  0.7029,\n",
    "#  0.667741935483871,\n",
    "#  0.667741935483871,\n",
    "#  1.0,\n",
    "#  0.0,\n",
    "#  0.8007736943907157)\n",
    "# Diabetes: trainable, dense 128. Training time: 29084.541052103043\n",
    "# (0.4852,\n",
    "#  0.653,\n",
    "#  0.667741935483871,\n",
    "#  0.667741935483871,\n",
    "#  1.0,\n",
    "#  0.0,\n",
    "#  0.8007736943907157)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
